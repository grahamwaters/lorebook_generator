[
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "uuid",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "uuid",
        "description": "uuid",
        "detail": "uuid",
        "documentation": {}
    },
    {
        "label": "nltk",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "nltk",
        "description": "nltk",
        "detail": "nltk",
        "documentation": {}
    },
    {
        "label": "sent_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "sent_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "sent_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "sent_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "sent_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "wikipedia",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "wikipedia",
        "description": "wikipedia",
        "detail": "wikipedia",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "helpers",
        "description": "helpers",
        "isExtraImport": true,
        "detail": "helpers",
        "documentation": {}
    },
    {
        "label": "pos_tag",
        "importPath": "nltk.tag",
        "description": "nltk.tag",
        "isExtraImport": true,
        "detail": "nltk.tag",
        "documentation": {}
    },
    {
        "label": "name",
        "importPath": "unicodedata",
        "description": "unicodedata",
        "isExtraImport": true,
        "detail": "unicodedata",
        "documentation": {}
    },
    {
        "label": "datetime",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datetime",
        "description": "datetime",
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "alive_bar",
        "importPath": "alive_progress",
        "description": "alive_progress",
        "isExtraImport": true,
        "detail": "alive_progress",
        "documentation": {}
    },
    {
        "label": "alive_bar",
        "importPath": "alive_progress",
        "description": "alive_progress",
        "isExtraImport": true,
        "detail": "alive_progress",
        "documentation": {}
    },
    {
        "label": "limits",
        "importPath": "ratelimit",
        "description": "ratelimit",
        "isExtraImport": true,
        "detail": "ratelimit",
        "documentation": {}
    },
    {
        "label": "sleep_and_retry",
        "importPath": "ratelimit",
        "description": "ratelimit",
        "isExtraImport": true,
        "detail": "ratelimit",
        "documentation": {}
    },
    {
        "label": "sleep_and_retry",
        "importPath": "ratelimit",
        "description": "ratelimit",
        "isExtraImport": true,
        "detail": "ratelimit",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "lorebook_from_year",
        "importPath": "01_lorebook_from_year",
        "description": "01_lorebook_from_year",
        "isExtraImport": true,
        "detail": "01_lorebook_from_year",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "scripts.01_lorebook_from_year",
        "description": "scripts.01_lorebook_from_year",
        "peekOfCode": "def main():\n    # generate a lorebook.lorebook file from the articles (text files) in the wikipedia_pages directory.\n    # Files:\n    # lorebook_example.lorebook - the example lorebook file\n    # lorebook_generated.lorebook - the generated lorebook file\n    # characters.csv - a list of characters to generate entries for (one per line)\n    # wikipedia_pages - a directory containing the text files of wikipedia articles to generate entries from (one per file), entry name will be the filename\n    # read in the lorebook_generated.lorebook file (if it exists)\n    try:\n        with open(\"./supporting_files/lorebook_generated.lorebook\", \"r\") as f:",
        "detail": "scripts.01_lorebook_from_year",
        "documentation": {}
    },
    {
        "label": "file_mode",
        "kind": 2,
        "importPath": "scripts.01_lorebook_from_year",
        "description": "scripts.01_lorebook_from_year",
        "peekOfCode": "def file_mode(mode, topics_scanner_list, years_list):\n    global entries\n    global entry_names\n    global ids\n    # global mode\n    print(\"Scanning for topics...\")\n    for filename in tqdm(os.listdir(\"./wikipedia_pages\")):\n        if filename == \".DS_Store\":\n            continue  # skip the .DS_Store file\n        with open(f\"./wikipedia_pages/{filename}\", \"r\") as f:",
        "detail": "scripts.01_lorebook_from_year",
        "documentation": {}
    },
    {
        "label": "web_mode",
        "kind": 2,
        "importPath": "scripts.01_lorebook_from_year",
        "description": "scripts.01_lorebook_from_year",
        "peekOfCode": "def web_mode(entry_names):\n    # use the wikipedia api to get the text of the wikipedia pages instead of reading them from files.\n    # this is slower, but it will get the latest version of the page.\n    global entries\n    global ids\n    global topics_scanner_list\n    global years_list\n    global mode\n    print(\"Scanning for topics...\")\n    for character in tqdm(entry_names):",
        "detail": "scripts.01_lorebook_from_year",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": "scripts.01_lorebook_from_year",
        "description": "scripts.01_lorebook_from_year",
        "peekOfCode": "stop_words = set(stopwords.words(\"english\"))\nmaxlinksperpage = 30\n# * Import Configuration Dictionary from scripts/lorebook_params.py file\n# goal: eventually have an external file for these variables\ncontext_config = {\n    \"prefix\": \"\",\n    \"suffix\": \"\\n\",\n    \"tokenBudget\": 200,  # max 2048\n    \"reservedTokens\": 0,\n    \"budgetPriority\": 400,",
        "detail": "scripts.01_lorebook_from_year",
        "documentation": {}
    },
    {
        "label": "maxlinksperpage",
        "kind": 5,
        "importPath": "scripts.01_lorebook_from_year",
        "description": "scripts.01_lorebook_from_year",
        "peekOfCode": "maxlinksperpage = 30\n# * Import Configuration Dictionary from scripts/lorebook_params.py file\n# goal: eventually have an external file for these variables\ncontext_config = {\n    \"prefix\": \"\",\n    \"suffix\": \"\\n\",\n    \"tokenBudget\": 200,  # max 2048\n    \"reservedTokens\": 0,\n    \"budgetPriority\": 400,\n    \"trimDirection\": \"trimBottom\",",
        "detail": "scripts.01_lorebook_from_year",
        "documentation": {}
    },
    {
        "label": "context_config",
        "kind": 5,
        "importPath": "scripts.01_lorebook_from_year",
        "description": "scripts.01_lorebook_from_year",
        "peekOfCode": "context_config = {\n    \"prefix\": \"\",\n    \"suffix\": \"\\n\",\n    \"tokenBudget\": 200,  # max 2048\n    \"reservedTokens\": 0,\n    \"budgetPriority\": 400,\n    \"trimDirection\": \"trimBottom\",\n    \"insertionType\": \"newline\",\n    \"maximumTrimType\": \"sentence\",\n    \"insertionPosition\": -1,",
        "detail": "scripts.01_lorebook_from_year",
        "documentation": {}
    },
    {
        "label": "preprocess",
        "kind": 2,
        "importPath": "scripts.02_lorebook_from_downloaded_wiki_articles",
        "description": "scripts.02_lorebook_from_downloaded_wiki_articles",
        "peekOfCode": "def preprocess(sent):\n    sent = nltk.word_tokenize(sent)\n    sent = nltk.pos_tag(sent)\n    return sent\ndef get_the_entities(content):\n    # get the entities from the text\n    entities = []\n    for sent in sent_tokenize(content):\n        for chunk in nltk.ne_chunk(preprocess(sent)):\n            if hasattr(chunk, \"label\"):",
        "detail": "scripts.02_lorebook_from_downloaded_wiki_articles",
        "documentation": {}
    },
    {
        "label": "get_the_entities",
        "kind": 2,
        "importPath": "scripts.02_lorebook_from_downloaded_wiki_articles",
        "description": "scripts.02_lorebook_from_downloaded_wiki_articles",
        "peekOfCode": "def get_the_entities(content):\n    # get the entities from the text\n    entities = []\n    for sent in sent_tokenize(content):\n        for chunk in nltk.ne_chunk(preprocess(sent)):\n            if hasattr(chunk, \"label\"):\n                entities.append(\" \".join(c[0] for c in chunk.leaves()))\n    return entities\ndef generate_entries_from_list(list_of_names):\n    # enter a list of people's names and get a list of entries, from wikipedia",
        "detail": "scripts.02_lorebook_from_downloaded_wiki_articles",
        "documentation": {}
    },
    {
        "label": "generate_entries_from_list",
        "kind": 2,
        "importPath": "scripts.02_lorebook_from_downloaded_wiki_articles",
        "description": "scripts.02_lorebook_from_downloaded_wiki_articles",
        "peekOfCode": "def generate_entries_from_list(list_of_names):\n    # enter a list of people's names and get a list of entries, from wikipedia\n    entries = []\n    entry_names = []\n    entry_keywords = []\n    for name in tqdm(list_of_names):\n        if name != \"\":\n            try:\n                entry = wikipedia.search(name)[\n                    0",
        "detail": "scripts.02_lorebook_from_downloaded_wiki_articles",
        "documentation": {}
    },
    {
        "label": "prev",
        "kind": 2,
        "importPath": "scripts.02_lorebook_from_downloaded_wiki_articles",
        "description": "scripts.02_lorebook_from_downloaded_wiki_articles",
        "peekOfCode": "def prev():\n    with open(\"lorebook_generated.lorebook\") as f:\n        lore_dict = json.load(f)\n    topics_list = []\n    entry_keys = []\n    # input_text = 'start'\n    # while input_text != '':\n    #     input_text = input('Enter a topic: ')\n    #     topics_list.append(input_text)\n    # read in the list of topics from the characters.csv file",
        "detail": "scripts.02_lorebook_from_downloaded_wiki_articles",
        "documentation": {}
    },
    {
        "label": "create_keys",
        "kind": 2,
        "importPath": "scripts.02_lorebook_from_downloaded_wiki_articles",
        "description": "scripts.02_lorebook_from_downloaded_wiki_articles",
        "peekOfCode": "def create_keys(entries):\n    # create a list of the keys for each entry (all proper nouns, places and dates)\n    keys = []\n    try:  # try to open the keys dict csv file\n        keys_dict = pd.read_csv(\"keys_dict.csv\").set_index(\"id\").to_dict()[\"keys\"]\n        entry_id = len(keys_dict)\n    except Exception as e:\n        print(e)\n        keys_dict = {}\n        entry_id = 0",
        "detail": "scripts.02_lorebook_from_downloaded_wiki_articles",
        "documentation": {}
    },
    {
        "label": "generate_lorebook",
        "kind": 2,
        "importPath": "scripts.02_lorebook_from_downloaded_wiki_articles",
        "description": "scripts.02_lorebook_from_downloaded_wiki_articles",
        "peekOfCode": "def generate_lorebook(lore_dict, entries, entry_names):\n    # generate a lorebook dictionary from the entries, entry_names, and ids\n    keys_dict = create_keys(\n        entries\n    )  # create the keys for each entry in the entries list\n    global context_config\n    for i in range(len(entries)):\n        # append blanks to lore_dict['entries'] to make room for the new entries\n        try:\n            lore_dict[\"entries\"][i] = {}",
        "detail": "scripts.02_lorebook_from_downloaded_wiki_articles",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "scripts.02_lorebook_from_downloaded_wiki_articles",
        "description": "scripts.02_lorebook_from_downloaded_wiki_articles",
        "peekOfCode": "def main():\n    # generate a lorebook.lorebook file from the articles (text files) in the wikipedia_pages directory.\n    # Files:\n    # lorebook_example.lorebook - the example lorebook file\n    # lorebook_generated.lorebook - the generated lorebook file\n    # characters.csv - a list of characters to generate entries for (one per line)\n    # wikipedia_pages - a directory containing the text files of wikipedia articles to generate entries from (one per file), entry name will be the filename\n    # read in the lorebook_generated.lorebook file (if it exists)\n    try:\n        with open(\"lorebook_generated.lorebook\", \"r\") as f:",
        "detail": "scripts.02_lorebook_from_downloaded_wiki_articles",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": "scripts.02_lorebook_from_downloaded_wiki_articles",
        "description": "scripts.02_lorebook_from_downloaded_wiki_articles",
        "peekOfCode": "stop_words = set(stopwords.words(\"english\"))\n# Functions\ndef preprocess(sent):\n    sent = nltk.word_tokenize(sent)\n    sent = nltk.pos_tag(sent)\n    return sent\ndef get_the_entities(content):\n    # get the entities from the text\n    entities = []\n    for sent in sent_tokenize(content):",
        "detail": "scripts.02_lorebook_from_downloaded_wiki_articles",
        "documentation": {}
    },
    {
        "label": "context_config",
        "kind": 5,
        "importPath": "scripts.02_lorebook_from_downloaded_wiki_articles",
        "description": "scripts.02_lorebook_from_downloaded_wiki_articles",
        "peekOfCode": "context_config = {\n    \"prefix\": \"\",\n    \"suffix\": \"\\n\",\n    \"tokenBudget\": 200,  # max 2048\n    \"reservedTokens\": 0,\n    \"budgetPriority\": 400,\n    \"trimDirection\": \"trimBottom\",\n    \"insertionType\": \"newline\",\n    \"maximumTrimType\": \"sentence\",\n    \"insertionPosition\": -1,",
        "detail": "scripts.02_lorebook_from_downloaded_wiki_articles",
        "documentation": {}
    },
    {
        "label": "examine_dates",
        "kind": 2,
        "importPath": "scripts.03_lorebook_from_wiki_withpulls",
        "description": "scripts.03_lorebook_from_wiki_withpulls",
        "peekOfCode": "def examine_dates(entry1, entry2):\n    # an article is useful if most of the dates in article A, fall within the max and min dates of article B with an error margin of 10 years.\n    article_one_dates = []\n    article_two_dates = []\n    for sent in sent_tokenize(entry1):\n        for chunk in nltk.ne_chunk(preprocess(sent)):\n            if hasattr(chunk, \"label\"):\n                if chunk.label() == \"DATE\":\n                    article_one_dates.append(\" \".join(c[0] for c in chunk.leaves()))\n    for sent in sent_tokenize(entry2):",
        "detail": "scripts.03_lorebook_from_wiki_withpulls",
        "documentation": {}
    },
    {
        "label": "preprocess",
        "kind": 2,
        "importPath": "scripts.03_lorebook_from_wiki_withpulls",
        "description": "scripts.03_lorebook_from_wiki_withpulls",
        "peekOfCode": "def preprocess(sent):\n    sent = nltk.word_tokenize(sent)\n    sent = nltk.pos_tag(sent)\n    return sent\n#todo:ticket 0001 - add a function to reduce keywords to a max of 50.\n# get the top most unique keywords for the entry as compared to the other entries\ndef find_unique_keys(keys_dict):\n    # the top most unique keywords for the entry as compared to the other entries in the lorebook\n    # the keys_dict is a dictionary with all keywords in it.\n    # each row is an entry, and each column is a keyword",
        "detail": "scripts.03_lorebook_from_wiki_withpulls",
        "documentation": {}
    },
    {
        "label": "find_unique_keys",
        "kind": 2,
        "importPath": "scripts.03_lorebook_from_wiki_withpulls",
        "description": "scripts.03_lorebook_from_wiki_withpulls",
        "peekOfCode": "def find_unique_keys(keys_dict):\n    # the top most unique keywords for the entry as compared to the other entries in the lorebook\n    # the keys_dict is a dictionary with all keywords in it.\n    # each row is an entry, and each column is a keyword\n    # get all the keywords (across all entries) -> keyword_master_list\n    # go through each row's keywords and keep them in one of two cases (1) if they occur only once in keyword_master_list or (2) if they occur more than once but the number of keywords currently saved is less than 50 and we have already checked all the keywords in the row for uniqueness.\n    # then we go to the next row and repeat the process.\n    # code:\n    # consolidate all the keywords into one list (from all the entries in keys_dict)\n    keyword_master_list = []",
        "detail": "scripts.03_lorebook_from_wiki_withpulls",
        "documentation": {}
    },
    {
        "label": "get_the_entities",
        "kind": 2,
        "importPath": "scripts.03_lorebook_from_wiki_withpulls",
        "description": "scripts.03_lorebook_from_wiki_withpulls",
        "peekOfCode": "def get_the_entities(content):\n    # get the entities from the text\n    entities = []\n    for sent in sent_tokenize(content):\n        for chunk in nltk.ne_chunk(preprocess(sent)):\n            if hasattr(chunk, \"label\"):\n                entities.append(\" \".join(c[0] for c in chunk.leaves()))\n    return entities\nperiod = 300 # 5 minutes\n@sleep_and_retry",
        "detail": "scripts.03_lorebook_from_wiki_withpulls",
        "documentation": {}
    },
    {
        "label": "inner_generator",
        "kind": 2,
        "importPath": "scripts.03_lorebook_from_wiki_withpulls",
        "description": "scripts.03_lorebook_from_wiki_withpulls",
        "peekOfCode": "def inner_generator(name,entries,entry_keywords,entry_names,bar):\n    if name != \"\":\n        try:\n            entry = wikipedia.search(name)[\n                0\n            ]  # get the first result from wikipedia, which is usually the most relevant\n            page = wikipedia.page(entry)\n            entry = page.content\n            entry = re.sub(r\"\\([^)]*\\)\", \" \", entry)  # remove anything in brackets\n            # strip the text of all special characters, and any escaped characters using regex",
        "detail": "scripts.03_lorebook_from_wiki_withpulls",
        "documentation": {}
    },
    {
        "label": "generate_entries_from_list",
        "kind": 2,
        "importPath": "scripts.03_lorebook_from_wiki_withpulls",
        "description": "scripts.03_lorebook_from_wiki_withpulls",
        "peekOfCode": "def generate_entries_from_list(list_of_names,bar):\n    # enter a list of people's names and get a list of entries, from wikipedia\n    entries = []\n    entry_names = []\n    entry_keywords = []\n    with alive_bar(len(list_of_names),bar=bar) as bar2:\n        for name in list_of_names:\n            # pause every 10 iterations to avoid getting blocked by wikipedia (for a random number of seconds)\n            # if list_of_names.index(name) % rest_time == 0:\n            #     waittime = random.randint(2, 6) # wait between 4 and 15 seconds",
        "detail": "scripts.03_lorebook_from_wiki_withpulls",
        "documentation": {}
    },
    {
        "label": "check_json_for_entry",
        "kind": 2,
        "importPath": "scripts.03_lorebook_from_wiki_withpulls",
        "description": "scripts.03_lorebook_from_wiki_withpulls",
        "peekOfCode": "def check_json_for_entry(entry_name, json_file):\n    # check if an entry already exists in the json file\n    with open(json_file, \"r\") as f:\n        data = json.load(f)\n    for entry in range(len(data[\"entries\"])):\n        if entry_name == \"nan\" or entry_name == \"\":\n            continue\n        if data[\"entries\"][entry][\"displayName\"] == entry_name:\n            print(f\"{entry_name} - entry already exists\", datetime.datetime.now())\n            return True",
        "detail": "scripts.03_lorebook_from_wiki_withpulls",
        "documentation": {}
    },
    {
        "label": "clear_the_lorebook",
        "kind": 2,
        "importPath": "scripts.03_lorebook_from_wiki_withpulls",
        "description": "scripts.03_lorebook_from_wiki_withpulls",
        "peekOfCode": "def clear_the_lorebook():\n    # reset the lorebook to an empty dictionary keeping the same format\n    # use the text in the /supporting_files/starter.lorebook file\n    with open(\"./supporting_files/starter.lorebook\", \"r\") as f:\n        lore_dict = json.load(f)\n    with open(\"./supporting_files/lorebook_generated.lorebook\", \"w+\") as f:\n        json.dump(lore_dict, f, indent=4)\n    print(\"Lorebook cleared\")\ndef clear_all_previously_saved_files():\n    # delete all files in the wikipedia_pages folder",
        "detail": "scripts.03_lorebook_from_wiki_withpulls",
        "documentation": {}
    },
    {
        "label": "clear_all_previously_saved_files",
        "kind": 2,
        "importPath": "scripts.03_lorebook_from_wiki_withpulls",
        "description": "scripts.03_lorebook_from_wiki_withpulls",
        "peekOfCode": "def clear_all_previously_saved_files():\n    # delete all files in the wikipedia_pages folder\n    for filename in os.listdir(\"wikipedia_pages\"):\n        os.remove(f\"wikipedia_pages/{filename}\")\ndef main():\n    \"\"\"\n    main function - runs the program, processing the names provided in the characters.csv file and adding them to the lorebook_generated.lorebook file in the supporting_files folder. The function uses the generate_entries_from_list function to generate the entries, and then adds them to the lorebook_generated.lorebook file.\n    \"\"\"\n    global context_config\n    global minimum_key_occurrences",
        "detail": "scripts.03_lorebook_from_wiki_withpulls",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "scripts.03_lorebook_from_wiki_withpulls",
        "description": "scripts.03_lorebook_from_wiki_withpulls",
        "peekOfCode": "def main():\n    \"\"\"\n    main function - runs the program, processing the names provided in the characters.csv file and adding them to the lorebook_generated.lorebook file in the supporting_files folder. The function uses the generate_entries_from_list function to generate the entries, and then adds them to the lorebook_generated.lorebook file.\n    \"\"\"\n    global context_config\n    global minimum_key_occurrences\n    global chunk_size # the number of names to process in each chunk\n    # open the lorebook_generated.lorebook file\n    try:\n        with open(\"./supporting_files/lorebook_generated.lorebook\") as f:",
        "detail": "scripts.03_lorebook_from_wiki_withpulls",
        "documentation": {}
    },
    {
        "label": "final_checks",
        "kind": 2,
        "importPath": "scripts.03_lorebook_from_wiki_withpulls",
        "description": "scripts.03_lorebook_from_wiki_withpulls",
        "peekOfCode": "def final_checks(entries, entry_names, entry_keywords, lore_dict, list_of_names,ids):\n    # #assert -- make sure the lengths are the same\n    #assert(len(entries) == len(entry_names), \"after removing existing entries lengths are not the same\")\n    #assert(len(entries) == len(entry_keywords), \"after removing existing entries lengths are not the same\")\n    #assert(len(entries) == len(ids), \"after removing existing entries lengths are not the same\")\n    #assert(len(entry_names) == len(entry_keywords), \"after removing existing entries lengths are not the same\")\n    # check that the name of each entry (i.e. 'Florence Nightingale') is in the first paragraph of the entry's text. If not, then raise an error\n    for entry, entry_name in zip(entries, entry_names):\n        if entry_name not in entry.split('\\n')[0]:\n            raise Exception(f\"{entry_name} is not in the first paragraph of the entry\")",
        "detail": "scripts.03_lorebook_from_wiki_withpulls",
        "documentation": {}
    },
    {
        "label": "rest_time",
        "kind": 5,
        "importPath": "scripts.03_lorebook_from_wiki_withpulls",
        "description": "scripts.03_lorebook_from_wiki_withpulls",
        "peekOfCode": "rest_time = 10 # max number of calls before resting\nchunk_size = 10 # number of entries per chunk\nwarnings.filterwarnings(\n    \"ignore\"\n)  # reason we are ignoring the warning is because we are using the wikipedia package to get the content of the articles but we don't mind if we miss a few along the way. As it is right now, the process is designed to be slightly imperfect.\n# Global Variables Declaration ------------------------------------------------\n# get the list of names from the topics file\nnltk.download(\"stopwords\")  # & download stopwords\nstop_words = set(stopwords.words(\"english\"))\nmaxlinksperpage = 30 # how many randomly sampled links to get from each page, to add to the list of keywords",
        "detail": "scripts.03_lorebook_from_wiki_withpulls",
        "documentation": {}
    },
    {
        "label": "chunk_size",
        "kind": 5,
        "importPath": "scripts.03_lorebook_from_wiki_withpulls",
        "description": "scripts.03_lorebook_from_wiki_withpulls",
        "peekOfCode": "chunk_size = 10 # number of entries per chunk\nwarnings.filterwarnings(\n    \"ignore\"\n)  # reason we are ignoring the warning is because we are using the wikipedia package to get the content of the articles but we don't mind if we miss a few along the way. As it is right now, the process is designed to be slightly imperfect.\n# Global Variables Declaration ------------------------------------------------\n# get the list of names from the topics file\nnltk.download(\"stopwords\")  # & download stopwords\nstop_words = set(stopwords.words(\"english\"))\nmaxlinksperpage = 30 # how many randomly sampled links to get from each page, to add to the list of keywords\nminimum_key_occurrences = 4  # minimum number of times a keyword must appear in the text to be considered a keyword",
        "detail": "scripts.03_lorebook_from_wiki_withpulls",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": "scripts.03_lorebook_from_wiki_withpulls",
        "description": "scripts.03_lorebook_from_wiki_withpulls",
        "peekOfCode": "stop_words = set(stopwords.words(\"english\"))\nmaxlinksperpage = 30 # how many randomly sampled links to get from each page, to add to the list of keywords\nminimum_key_occurrences = 4  # minimum number of times a keyword must appear in the text to be considered a keyword\ncontext_config = {\n    \"prefix\": \"\",\n    \"suffix\": \"\\n\",\n    \"tokenBudget\": 100,  # max 2048\n    \"reservedTokens\": 0,\n    \"budgetPriority\": 400,\n    \"trimDirection\": \"trimBottom\",",
        "detail": "scripts.03_lorebook_from_wiki_withpulls",
        "documentation": {}
    },
    {
        "label": "maxlinksperpage",
        "kind": 5,
        "importPath": "scripts.03_lorebook_from_wiki_withpulls",
        "description": "scripts.03_lorebook_from_wiki_withpulls",
        "peekOfCode": "maxlinksperpage = 30 # how many randomly sampled links to get from each page, to add to the list of keywords\nminimum_key_occurrences = 4  # minimum number of times a keyword must appear in the text to be considered a keyword\ncontext_config = {\n    \"prefix\": \"\",\n    \"suffix\": \"\\n\",\n    \"tokenBudget\": 100,  # max 2048\n    \"reservedTokens\": 0,\n    \"budgetPriority\": 400,\n    \"trimDirection\": \"trimBottom\",\n    \"insertionType\": \"newline\",",
        "detail": "scripts.03_lorebook_from_wiki_withpulls",
        "documentation": {}
    },
    {
        "label": "minimum_key_occurrences",
        "kind": 5,
        "importPath": "scripts.03_lorebook_from_wiki_withpulls",
        "description": "scripts.03_lorebook_from_wiki_withpulls",
        "peekOfCode": "minimum_key_occurrences = 4  # minimum number of times a keyword must appear in the text to be considered a keyword\ncontext_config = {\n    \"prefix\": \"\",\n    \"suffix\": \"\\n\",\n    \"tokenBudget\": 100,  # max 2048\n    \"reservedTokens\": 0,\n    \"budgetPriority\": 400,\n    \"trimDirection\": \"trimBottom\",\n    \"insertionType\": \"newline\",\n    \"maximumTrimType\": \"sentence\",",
        "detail": "scripts.03_lorebook_from_wiki_withpulls",
        "documentation": {}
    },
    {
        "label": "context_config",
        "kind": 5,
        "importPath": "scripts.03_lorebook_from_wiki_withpulls",
        "description": "scripts.03_lorebook_from_wiki_withpulls",
        "peekOfCode": "context_config = {\n    \"prefix\": \"\",\n    \"suffix\": \"\\n\",\n    \"tokenBudget\": 100,  # max 2048\n    \"reservedTokens\": 0,\n    \"budgetPriority\": 400,\n    \"trimDirection\": \"trimBottom\",\n    \"insertionType\": \"newline\",\n    \"maximumTrimType\": \"sentence\",\n    \"insertionPosition\": -1,",
        "detail": "scripts.03_lorebook_from_wiki_withpulls",
        "documentation": {}
    },
    {
        "label": "period",
        "kind": 5,
        "importPath": "scripts.03_lorebook_from_wiki_withpulls",
        "description": "scripts.03_lorebook_from_wiki_withpulls",
        "peekOfCode": "period = 300 # 5 minutes\n@sleep_and_retry\n#@limits(calls=15, period=period) # 15 calls per 5 minutes\ndef inner_generator(name,entries,entry_keywords,entry_names,bar):\n    if name != \"\":\n        try:\n            entry = wikipedia.search(name)[\n                0\n            ]  # get the first result from wikipedia, which is usually the most relevant\n            page = wikipedia.page(entry)",
        "detail": "scripts.03_lorebook_from_wiki_withpulls",
        "documentation": {}
    },
    {
        "label": "preprocess_sentence",
        "kind": 2,
        "importPath": "scripts.04_get_relevant_subtopics",
        "description": "scripts.04_get_relevant_subtopics",
        "peekOfCode": "def preprocess_sentence(sent):\n    sent = nltk.word_tokenize(sent)\n    sent = nltk.pos_tag(sent)\n    return sent\ndef filename_create(page_title):\n    filename = page_title.replace(\" \", \"_\") # replace spaces with underscores\n    filename = filename.replace(\"/\", \"_\") # replace slashes with underscores\n    filename = filename.replace(\":\", \"_\") # replace colons with underscores\n    filename = filename.replace(\"?\", \"_\") # replace question marks with underscores\n    filename = filename.replace(\"*\", \"_\") # replace asterisks with underscores",
        "detail": "scripts.04_get_relevant_subtopics",
        "documentation": {}
    },
    {
        "label": "filename_create",
        "kind": 2,
        "importPath": "scripts.04_get_relevant_subtopics",
        "description": "scripts.04_get_relevant_subtopics",
        "peekOfCode": "def filename_create(page_title):\n    filename = page_title.replace(\" \", \"_\") # replace spaces with underscores\n    filename = filename.replace(\"/\", \"_\") # replace slashes with underscores\n    filename = filename.replace(\":\", \"_\") # replace colons with underscores\n    filename = filename.replace(\"?\", \"_\") # replace question marks with underscores\n    filename = filename.replace(\"*\", \"_\") # replace asterisks with underscores\n    filename = filename.replace('\"', \"\") # replace double quotes with underscores\n    filename = filename.replace(\"'\", \"\") # replace single quotes with underscores\n    filename = filename.replace(\"<\", \"_\") # replace less than signs with underscores\n    filename = filename.replace(\">\", \"_\") # replace greater than signs with underscores",
        "detail": "scripts.04_get_relevant_subtopics",
        "documentation": {}
    },
    {
        "label": "while_page_exists",
        "kind": 2,
        "importPath": "scripts.04_get_relevant_subtopics",
        "description": "scripts.04_get_relevant_subtopics",
        "peekOfCode": "def while_page_exists(page,filename):\n    try:\n        page_text = page.content # get the content of the page\n        # save the page text to a file with the name of the page as the file name in the wikipedia_pages folder\n        # to save storage space, we should parse the page_text and remove all extra spaces.\n        page_text = re.sub(r\"\\s+\", \" \", page_text) # replace all extra spaces with a single space\n        with open(f'wikipedia_pages/{filename}.txt', 'w+') as f:\n            f.write(page_text)\n        print(f'Saved {filename} to file.', end=' ')\n        return True",
        "detail": "scripts.04_get_relevant_subtopics",
        "documentation": {}
    },
    {
        "label": "topic_check_pos_type",
        "kind": 2,
        "importPath": "scripts.04_get_relevant_subtopics",
        "description": "scripts.04_get_relevant_subtopics",
        "peekOfCode": "def topic_check_pos_type(topic):\n    # Using NLTK, determine what POS the topic word is and return True if it is a noun, False if it is not\n    # if the topic word is a noun, then we can use it as a keyword to search for subtopics\n    # if the topic word is not a noun, then we cannot use it as a keyword to search for subtopics\n    topic_processed = preprocess_sentence(topic) # preprocess the topic word\n    # Noun-phrase chunking to identify entities in the topic\n    pattern = r\"NP: {<DT|PP\\$>?<JJ>*<NN>}\"   # chunk determiner/possessive, adjectives and noun\n    cp = nltk.RegexpParser(pattern) # create the chunk parser\n    cs = cp.parse(topic_processed) # parse the topic\n    # if the entity is a person, country, organization, or historical event, then keep going otherwise return False",
        "detail": "scripts.04_get_relevant_subtopics",
        "documentation": {}
    },
    {
        "label": "book_keeper_bot",
        "kind": 2,
        "importPath": "scripts.04_get_relevant_subtopics",
        "description": "scripts.04_get_relevant_subtopics",
        "peekOfCode": "def book_keeper_bot(topic):\n    # this function will check if the topic has already been saved to the wikipedia_pages folder\n    # if it has, then it will return True\n    # if it has not, then it will return False\n    filename = filename_create(topic) # create the filename\n    if os.path.exists(f'./wikipedia_keys/{filename}.csv'): # if the file exists, then return True\n        return True\n    else:\n        return False\n#check if the topic has a file in the wikipedia_pages or wikipedia_keys folder",
        "detail": "scripts.04_get_relevant_subtopics",
        "documentation": {}
    },
    {
        "label": "get_links",
        "kind": 2,
        "importPath": "scripts.04_get_relevant_subtopics",
        "description": "scripts.04_get_relevant_subtopics",
        "peekOfCode": "def get_links(topic,all_topics):\n    # If the topic has already been saved to the wikipedia_pages folder, then skip it\n    status = book_keeper_bot(topic) # check if the topic has already been saved to the wikipedia_pages folder\n    #//status = book_keeper(topic) # check if the topic has already been saved to the wikipedia_pages folder\n    if status == True: # if the topic has already been saved to the wikipedia_pages folder, then skip it\n        print(f'{topic} has already been saved to the wikipedia_pages folder.',end=' -> ')\n        # open the file and get the links\n        filename = filename_create(topic) # create the filename\n        with open(f'./wikipedia_keys/{filename}.csv', 'r') as f:\n            links = f.read().splitlines() # read the file and split the lines into a list",
        "detail": "scripts.04_get_relevant_subtopics",
        "documentation": {}
    },
    {
        "label": "get_relevant_subtopics",
        "kind": 2,
        "importPath": "scripts.04_get_relevant_subtopics",
        "description": "scripts.04_get_relevant_subtopics",
        "peekOfCode": "def get_relevant_subtopics(parent_topic):\n    # get the list of subtopics from the parent topic page.\n    # get all links from the parent topic page\n    global keys_dict\n    # make sure that the topic page will show up in the search results\n    parent_topic_links = get_links(parent_topic,parent_topic)\n    #note: could use a for loop above until len(parent_topic_links) > 0\n    # get the list of subtopics from the parent topic page.\n    subtopics = []\n    # if parent_topic_links is NoneType then check the keys_dict.csv file for the topic",
        "detail": "scripts.04_get_relevant_subtopics",
        "documentation": {}
    },
    {
        "label": "divide_into_segments",
        "kind": 2,
        "importPath": "scripts.04_get_relevant_subtopics",
        "description": "scripts.04_get_relevant_subtopics",
        "peekOfCode": "def divide_into_segments(page_file_content):\n    # goal:\n    # 1. divide the page into segments based on the pattern '== History ==' where History is the name of a section, and the page_name is the name of the dictionary entry.\n    # 2. Save each segment to the dictionary entry for the page_name using the section name as the key.\n    # 3. Return the dictionary entry for the page_name.\n    # 4. If the page does not have any sections, then save the entire page as the value for the key 'content'.\n    # get the list of sections\n    sections = re.findall(r\"==\\s*(.*?)\\s*==\", page_file_content)\n    # get the list of section contents\n    section_contents = re.split(r\"==\\s*(.*?)\\s*==\", page_file_content)",
        "detail": "scripts.04_get_relevant_subtopics",
        "documentation": {}
    },
    {
        "label": "add_to_master_dict",
        "kind": 2,
        "importPath": "scripts.04_get_relevant_subtopics",
        "description": "scripts.04_get_relevant_subtopics",
        "peekOfCode": "def add_to_master_dict(subdict):\n    # given the results of divide_into_segments, add the subdict to the master dictionary\n    # if the page_name is already in the master dictionary, then add the subdict to the existing dictionary entry\n    # if the page_name is not in the master dictionary, then add the subdict as a new dictionary entry\n    global master_dict\n    for page_name in subdict:\n        if page_name in master_dict:\n            master_dict[page_name].update(subdict[page_name])\n        else:\n            master_dict[page_name] = subdict[page_name]",
        "detail": "scripts.04_get_relevant_subtopics",
        "documentation": {}
    },
    {
        "label": "clear_all_previously_saved_files",
        "kind": 2,
        "importPath": "scripts.04_get_relevant_subtopics",
        "description": "scripts.04_get_relevant_subtopics",
        "peekOfCode": "def clear_all_previously_saved_files():\n    # delete all files in the wikipedia_pages folder\n    for filename in os.listdir(\"wikipedia_pages\"):\n        os.remove(f\"wikipedia_pages/{filename}\")\ndef main():\n    global N\n    global master_dict\n    print(\"\\n\\nWelcome to the Subtopic Finder!\")\n    print(\"This program will find subtopics for a given topic.\")\n    print(\"All subtopics will be saved (article text to the wikipedia_pages directory) and added to the master dictionary.\")",
        "detail": "scripts.04_get_relevant_subtopics",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "scripts.04_get_relevant_subtopics",
        "description": "scripts.04_get_relevant_subtopics",
        "peekOfCode": "def main():\n    global N\n    global master_dict\n    print(\"\\n\\nWelcome to the Subtopic Finder!\")\n    print(\"This program will find subtopics for a given topic.\")\n    print(\"All subtopics will be saved (article text to the wikipedia_pages directory) and added to the master dictionary.\")\n    choice = input(\"\\n  Would you like to clear previously generated articles? (y/n) \")\n    if choice == \"y\":\n        clear_all_previously_saved_files()\n    print(\"Settings (Current):\")",
        "detail": "scripts.04_get_relevant_subtopics",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": "scripts.04_get_relevant_subtopics",
        "description": "scripts.04_get_relevant_subtopics",
        "peekOfCode": "stop_words = set(stopwords.words(\"english\"))\nmaxlinksperpage = 30\nminimum_key_occurrences = 4  # minimum number of times a keyword must appear in the text to be considered a keyword\ncontext_config = {\n    \"prefix\": \"\",\n    \"suffix\": \"\\n\",\n    \"tokenBudget\": 100,  # max 2048\n    \"reservedTokens\": 0,\n    \"budgetPriority\": 400,\n    \"trimDirection\": \"trimBottom\",",
        "detail": "scripts.04_get_relevant_subtopics",
        "documentation": {}
    },
    {
        "label": "maxlinksperpage",
        "kind": 5,
        "importPath": "scripts.04_get_relevant_subtopics",
        "description": "scripts.04_get_relevant_subtopics",
        "peekOfCode": "maxlinksperpage = 30\nminimum_key_occurrences = 4  # minimum number of times a keyword must appear in the text to be considered a keyword\ncontext_config = {\n    \"prefix\": \"\",\n    \"suffix\": \"\\n\",\n    \"tokenBudget\": 100,  # max 2048\n    \"reservedTokens\": 0,\n    \"budgetPriority\": 400,\n    \"trimDirection\": \"trimBottom\",\n    \"insertionType\": \"newline\",",
        "detail": "scripts.04_get_relevant_subtopics",
        "documentation": {}
    },
    {
        "label": "minimum_key_occurrences",
        "kind": 5,
        "importPath": "scripts.04_get_relevant_subtopics",
        "description": "scripts.04_get_relevant_subtopics",
        "peekOfCode": "minimum_key_occurrences = 4  # minimum number of times a keyword must appear in the text to be considered a keyword\ncontext_config = {\n    \"prefix\": \"\",\n    \"suffix\": \"\\n\",\n    \"tokenBudget\": 100,  # max 2048\n    \"reservedTokens\": 0,\n    \"budgetPriority\": 400,\n    \"trimDirection\": \"trimBottom\",\n    \"insertionType\": \"newline\",\n    \"maximumTrimType\": \"sentence\",",
        "detail": "scripts.04_get_relevant_subtopics",
        "documentation": {}
    },
    {
        "label": "context_config",
        "kind": 5,
        "importPath": "scripts.04_get_relevant_subtopics",
        "description": "scripts.04_get_relevant_subtopics",
        "peekOfCode": "context_config = {\n    \"prefix\": \"\",\n    \"suffix\": \"\\n\",\n    \"tokenBudget\": 100,  # max 2048\n    \"reservedTokens\": 0,\n    \"budgetPriority\": 400,\n    \"trimDirection\": \"trimBottom\",\n    \"insertionType\": \"newline\",\n    \"maximumTrimType\": \"sentence\",\n    \"insertionPosition\": -1,",
        "detail": "scripts.04_get_relevant_subtopics",
        "documentation": {}
    },
    {
        "label": "master_dict",
        "kind": 5,
        "importPath": "scripts.04_get_relevant_subtopics",
        "description": "scripts.04_get_relevant_subtopics",
        "peekOfCode": "master_dict = {} # get the list of keywords from the parent topic page.\n# save the keys from the keys_dict.csv file to a dict: keys_dict\nkeys_dict = {}\nwith open(\"./data/keys_dict.csv\", \"r\") as f:\n    for line in f:\n        line = line.strip()\n        line = line.split(\",\")\n        keys_dict[line[0]] = line[1]\n#*###########################################################################################################\n#& Functions",
        "detail": "scripts.04_get_relevant_subtopics",
        "documentation": {}
    },
    {
        "label": "keys_dict",
        "kind": 5,
        "importPath": "scripts.04_get_relevant_subtopics",
        "description": "scripts.04_get_relevant_subtopics",
        "peekOfCode": "keys_dict = {}\nwith open(\"./data/keys_dict.csv\", \"r\") as f:\n    for line in f:\n        line = line.strip()\n        line = line.split(\",\")\n        keys_dict[line[0]] = line[1]\n#*###########################################################################################################\n#& Functions\n#*###########################################################################################################\n#& Global Variables:",
        "detail": "scripts.04_get_relevant_subtopics",
        "documentation": {}
    },
    {
        "label": "N",
        "kind": 5,
        "importPath": "scripts.04_get_relevant_subtopics",
        "description": "scripts.04_get_relevant_subtopics",
        "peekOfCode": "N = 10 # number of words each child page should have in common with the parent page to be considered.\n#*############################################################################################################\n#^ Getting the relevant subtopics for a parent topic (e.g. \"Machine Learning\" for \"Artificial Intelligence\"). This is done by getting the list of subtopics from the Wikipedia page of the parent topic and then getting the list of keywords (links) from the Wikipedia page of each subtopic. The subtopic is then considered relevant if it has at least N keywords in common with the parent topic.\n##############################################################################################################\ndef preprocess_sentence(sent):\n    sent = nltk.word_tokenize(sent)\n    sent = nltk.pos_tag(sent)\n    return sent\ndef filename_create(page_title):\n    filename = page_title.replace(\" \", \"_\") # replace spaces with underscores",
        "detail": "scripts.04_get_relevant_subtopics",
        "documentation": {}
    },
    {
        "label": "preprocess",
        "kind": 2,
        "importPath": "scripts.helpers",
        "description": "scripts.helpers",
        "peekOfCode": "def preprocess(sent):\n    \"\"\"\n    preprocess the text to remove stopwords and punctuation\n    This function is used to preprocess the text before it is fed into the named entity recognition algorithm.\n    :param sent: the text to be preprocessed\n    :type sent: str\n    :return: the preprocessed text\n    :rtype: str\n    \"\"\"\n    sent = nltk.word_tokenize(sent)",
        "detail": "scripts.helpers",
        "documentation": {}
    },
    {
        "label": "get_the_entities",
        "kind": 2,
        "importPath": "scripts.helpers",
        "description": "scripts.helpers",
        "peekOfCode": "def get_the_entities(content):\n    \"\"\"\n    get_the_entities is a function that uses the nltk library to extract the named entities from a text.\n    This function uses the nltk library to extract the named entities from a text. It uses the nltk library to preprocess the text, and then uses the nltk library to extract the named entities from the text. It returns a list of the named entities.\n    :param content: the text to extract the named entities from\n    :type content:  str\n    :return: a list of the named entities\n    :rtype: list\n    \"\"\"\n    # get the entities from the text",
        "detail": "scripts.helpers",
        "documentation": {}
    },
    {
        "label": "generate_entries_from_list",
        "kind": 2,
        "importPath": "scripts.helpers",
        "description": "scripts.helpers",
        "peekOfCode": "def generate_entries_from_list(list_of_names):\n    # enter a list of people's names and get a list of entries, from wikipedia\n    \"\"\"\n    generate_entries_from_list is a function that generates a list of entries from a list of names.\n    This function generates a list of entries from a list of names. It uses the wikipedia library to get the wikipedia page for each name, and then extracts the text from the wikipedia page. It then returns a list of the entries.\n    :param list_of_names: a list of names\n    :type list_of_names: list\n    :return: a list of entries\n    :rtype: list\n    \"\"\"",
        "detail": "scripts.helpers",
        "documentation": {}
    },
    {
        "label": "create_keys",
        "kind": 2,
        "importPath": "scripts.helpers",
        "description": "scripts.helpers",
        "peekOfCode": "def create_keys(entries):\n    \"\"\"\n    create_keys is a function that creates a list of keys from a list of entries.\n    This function creates a list of keys from a list of entries. It uses the gensim library to create a list of keys from the entries. It returns a list of keys.\n    :param entries: a list of entries\n    :type entries: list\n    :return: a list of keys\n    :rtype: list\n    \"\"\"\n    # create a list of the keys for each entry (all proper nouns, places and dates)",
        "detail": "scripts.helpers",
        "documentation": {}
    },
    {
        "label": "generate_lorebook",
        "kind": 2,
        "importPath": "scripts.helpers",
        "description": "scripts.helpers",
        "peekOfCode": "def generate_lorebook(\n    lore_dict, characters, entries, entry_names, ids, years_list, topics_scanner_list\n):\n    # add the entries to the lorebook dictionary. All we have to change is the text, display name, create a random id, and add the keys (which are the words in the text). All other fields can be copied from the first entry.\n    keys_dict = create_keys(\n        entries\n    )  # create the keys for each entry in the entries list\n    global context_config\n    for i in range(len(entries)):\n        # append blanks to lore_dict['entries'] to make room for the new entries",
        "detail": "scripts.helpers",
        "documentation": {}
    },
    {
        "label": "preprocess",
        "kind": 2,
        "importPath": "shorter_03",
        "description": "shorter_03",
        "peekOfCode": "def preprocess(sent):\n  sent = nltk.word_tokenize(sent)\n  sent = nltk.pos_tag(sent)\n  return sent\ndef examine_dates(entry1, entry2):\n  article_one_dates = []\n  article_two_dates = []\n  for sent in sent_tokenize(entry1):\n    for chunk in nltk.ne_chunk(preprocess(sent)):\n      if hasattr(chunk, \"label\") and chunk.label() == \"DATE\":",
        "detail": "shorter_03",
        "documentation": {}
    },
    {
        "label": "examine_dates",
        "kind": 2,
        "importPath": "shorter_03",
        "description": "shorter_03",
        "peekOfCode": "def examine_dates(entry1, entry2):\n  article_one_dates = []\n  article_two_dates = []\n  for sent in sent_tokenize(entry1):\n    for chunk in nltk.ne_chunk(preprocess(sent)):\n      if hasattr(chunk, \"label\") and chunk.label() == \"DATE\":\n        article_one_dates.append(\" \".join(c[0] for c in chunk.leaves()))\n  for sent in sent_tokenize(entry2):\n    for chunk in nltk.ne_chunk(preprocess(sent)):\n      if hasattr(chunk, \"label\") and chunk.label() == \"DATE\":",
        "detail": "shorter_03",
        "documentation": {}
    },
    {
        "label": "find_unique_keys",
        "kind": 2,
        "importPath": "shorter_03",
        "description": "shorter_03",
        "peekOfCode": "def find_unique_keys(keys_dict):\n  # get all the keywords (across all entries) -> keyword_master_list\n  keyword_master_list = [key for entry in keys_dict for key in keys_dict[entry]]\n  # go through each row's keywords and keep them in one of two cases:\n  # (1) if they occur only once in keyword_master_list or (2) if they occur more often than the minimum threshold\n  unique_keys = []\n  for entry in keys_dict:\n    for key in keys_dict[entry]:\n      if keyword_master_list.count(key) == 1 or keyword_master_list.count(key) >= minimum_key_occurrences:\n        unique_keys.append(key)",
        "detail": "shorter_03",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": "shorter_03",
        "description": "shorter_03",
        "peekOfCode": "stop_words = set(stopwords.words(\"english\"))\nmaxlinksperpage = 30\nminimum_key_occurrences = 4",
        "detail": "shorter_03",
        "documentation": {}
    },
    {
        "label": "maxlinksperpage",
        "kind": 5,
        "importPath": "shorter_03",
        "description": "shorter_03",
        "peekOfCode": "maxlinksperpage = 30\nminimum_key_occurrences = 4",
        "detail": "shorter_03",
        "documentation": {}
    },
    {
        "label": "minimum_key_occurrences",
        "kind": 5,
        "importPath": "shorter_03",
        "description": "shorter_03",
        "peekOfCode": "minimum_key_occurrences = 4",
        "detail": "shorter_03",
        "documentation": {}
    }
]