[
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "wikipedia",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "wikipedia",
        "description": "wikipedia",
        "detail": "wikipedia",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tqdm",
        "description": "tqdm",
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "uuid",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "uuid",
        "description": "uuid",
        "detail": "uuid",
        "documentation": {}
    },
    {
        "label": "topics",
        "importPath": "pydoc_data.topics",
        "description": "pydoc_data.topics",
        "isExtraImport": true,
        "detail": "pydoc_data.topics",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "CountVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "nltk",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "nltk",
        "description": "nltk",
        "detail": "nltk",
        "documentation": {}
    },
    {
        "label": "sent_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "pos_tag",
        "importPath": "nltk.tag",
        "description": "nltk.tag",
        "isExtraImport": true,
        "detail": "nltk.tag",
        "documentation": {}
    },
    {
        "label": "datetime",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datetime",
        "description": "datetime",
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "icecream",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "icecream",
        "description": "icecream",
        "detail": "icecream",
        "documentation": {}
    },
    {
        "label": "ic",
        "importPath": "icecream",
        "description": "icecream",
        "isExtraImport": true,
        "detail": "icecream",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "json,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json.",
        "description": "json.",
        "detail": "json.",
        "documentation": {}
    },
    {
        "label": "check_json_for_entry",
        "importPath": "main",
        "description": "main",
        "isExtraImport": true,
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "entries",
        "importPath": "main",
        "description": "main",
        "isExtraImport": true,
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "master_list_of_names",
        "importPath": "main",
        "description": "main",
        "isExtraImport": true,
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "preprocess",
        "importPath": "main",
        "description": "main",
        "isExtraImport": true,
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "importPath": "main",
        "description": "main",
        "isExtraImport": true,
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "wikipedia_extractor",
        "kind": 2,
        "importPath": "lorebook_generator",
        "description": "lorebook_generator",
        "peekOfCode": "def wikipedia_extractor(names):\n    # given a person string (character), generate a lorebook for them from\n    # pages on wikipedia for the character. The lorebook will be a dictionary\n    lorebook = {}\n    # get the wikipedia page for each name in names\n    pages = []\n    iterator = 0\n    for name in names():\n        iterator += 1\n    iterator = 0",
        "detail": "lorebook_generator",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "lorebook_generator",
        "description": "lorebook_generator",
        "peekOfCode": "def main():\n    # there is a list of characters/topics in several files in the data folder\n    # we will use these to generate a lorebook for the novelai project\n    # read in the files\n    characters = pd.read_csv('entities_and_topics/characters.csv')\n    # generate a lorebook for each character\n    lorebook = wikipedia_extractor(characters)\n    # save the lorebook to a json file\n    lorebook.to_json('generated_lorebook.lorebook')\nif __name__ == '__main__':",
        "detail": "lorebook_generator",
        "documentation": {}
    },
    {
        "label": "context_config",
        "kind": 5,
        "importPath": "lorebook_generator",
        "description": "lorebook_generator",
        "peekOfCode": "context_config = {\n    \"prefix\": \"\",\n    \"suffix\": \"\\n\",\n    \"tokenBudget\": 100,  # max 2048\n    \"reservedTokens\": 0,\n    \"budgetPriority\": 400,\n    \"trimDirection\": \"trimBottom\",\n    \"insertionType\": \"newline\",\n    \"maximumTrimType\": \"sentence\",\n    \"insertionPosition\": -1,",
        "detail": "lorebook_generator",
        "documentation": {}
    },
    {
        "label": "preprocess",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def preprocess(sent):\n    sent = nltk.word_tokenize(sent)\n    sent = nltk.pos_tag(sent)\n    return sent\ndef get_the_entities(content):\n    ic()\n    # get the entities from the text\n    entities = []\n    for sent in sent_tokenize(content):\n        for chunk in nltk.ne_chunk(preprocess(sent)):",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "get_the_entities",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def get_the_entities(content):\n    ic()\n    # get the entities from the text\n    entities = []\n    for sent in sent_tokenize(content):\n        for chunk in nltk.ne_chunk(preprocess(sent)):\n            if hasattr(chunk, \"label\"):\n                entities.append(\" \".join(c[0] for c in chunk.leaves()))\n    return entities\ndef generate_entries_from_list(list_of_names):",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "generate_entries_from_list",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def generate_entries_from_list(list_of_names):\n    ic()\n    # enter a list of people's names and get a list of entries, from wikipedia\n    entries = []\n    entry_names = []\n    entry_keywords = []\n    for name in tqdm(list_of_names):\n        if name != \"\":\n            try:\n                entry = wikipedia.search(name)[",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "check_json_for_entry",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def check_json_for_entry(entry_name, json_file):\n    # check if an entry already exists in the json file\n    with open(json_file, \"r\") as f:\n        data = json.load(f)\n    for entry in range(len(data[\"entries\"])):\n        if data[\"entries\"][entry][\"displayName\"] == entry_name:\n            print(f\"{entry_name} - entry already exists\")\n            return True\n    return False\nfrom tqdm import tqdm",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "examine_dates",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def examine_dates(entry1, entry2):\n    # an article is useful if most of the dates in article A, fall within the max and min dates of article B with an error margin of 10 years.\n    ic()\n    article_one_dates = []\n    article_two_dates = []\n    for sent in sent_tokenize(entry1):\n        for chunk in nltk.ne_chunk(preprocess(sent)):\n            if hasattr(chunk, \"label\"):\n                if chunk.label() == \"DATE\":\n                    article_one_dates.append(\" \".join(c[0] for c in chunk.leaves()))",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def main():\n    ic()\n    # check those article pages for length (if they are too short, skip them)\n    # if they are long enough, and are not already in the list, add them to the list\n    list_of_names_f = pd.read_csv(\"characters.csv\")[\"Name\"].tolist()\n    # DROP NANs\n    list_of_names_f = [x for x in list_of_names_f if str(x) != \"nan\"]\n    # print(type(list_of_names))\n    # list_of_names = [x[0] for x in list_of_names.values.tolist()]\n    # only keep names in the list of names that are not already in the json file",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "stop_words = set(stopwords.words(\"english\"))\n# import the json file lorebook_example.lorebook\ndef preprocess(sent):\n    sent = nltk.word_tokenize(sent)\n    sent = nltk.pos_tag(sent)\n    return sent\ndef get_the_entities(content):\n    ic()\n    # get the entities from the text\n    entities = []",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "vectorizer",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "vectorizer = CountVectorizer()\nmaxlinksperpage = 50\ndef examine_dates(entry1, entry2):\n    # an article is useful if most of the dates in article A, fall within the max and min dates of article B with an error margin of 10 years.\n    ic()\n    article_one_dates = []\n    article_two_dates = []\n    for sent in sent_tokenize(entry1):\n        for chunk in nltk.ne_chunk(preprocess(sent)):\n            if hasattr(chunk, \"label\"):",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "maxlinksperpage",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "maxlinksperpage = 50\ndef examine_dates(entry1, entry2):\n    # an article is useful if most of the dates in article A, fall within the max and min dates of article B with an error margin of 10 years.\n    ic()\n    article_one_dates = []\n    article_two_dates = []\n    for sent in sent_tokenize(entry1):\n        for chunk in nltk.ne_chunk(preprocess(sent)):\n            if hasattr(chunk, \"label\"):\n                if chunk.label() == \"DATE\":",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "open_characters",
        "kind": 2,
        "importPath": "saver",
        "description": "saver",
        "peekOfCode": "def open_characters():\n    with open(\"characters.csv\", \"r\") as f:\n        characters = f.read().splitlines()\n    return characters\nlist_of_names_f = open_characters()\nwith open('lorebook_generated.lorebook') as f:\n    lore_dict = json.load(f)\ntopics_list = [x for x in list_of_names_f if not check_json_for_entry(x, 'lorebook_generated.lorebook')]\nentry_keys = []\nfor entry in entries:",
        "detail": "saver",
        "documentation": {}
    },
    {
        "label": "list_of_names_f",
        "kind": 5,
        "importPath": "saver",
        "description": "saver",
        "peekOfCode": "list_of_names_f = open_characters()\nwith open('lorebook_generated.lorebook') as f:\n    lore_dict = json.load(f)\ntopics_list = [x for x in list_of_names_f if not check_json_for_entry(x, 'lorebook_generated.lorebook')]\nentry_keys = []\nfor entry in entries:\n    topics_list.append(entry)\n    entry_keys.append(str(uuid.uuid4()))\nassert(type(topics_list) == list) # make sure it's a list\n# entries, entry_names = generate_entries_from_list(lore_dict['people'])",
        "detail": "saver",
        "documentation": {}
    },
    {
        "label": "topics_list",
        "kind": 5,
        "importPath": "saver",
        "description": "saver",
        "peekOfCode": "topics_list = [x for x in list_of_names_f if not check_json_for_entry(x, 'lorebook_generated.lorebook')]\nentry_keys = []\nfor entry in entries:\n    topics_list.append(entry)\n    entry_keys.append(str(uuid.uuid4()))\nassert(type(topics_list) == list) # make sure it's a list\n# entries, entry_names = generate_entries_from_list(lore_dict['people'])\n# generate only the entries in topics_list that are not already in the lorebook\n#existing_topics = [entry['name'] for entry in lore_dict['entries']]\n#topics_list = [x for x in topics_list if x not in lore_dict]",
        "detail": "saver",
        "documentation": {}
    },
    {
        "label": "entry_keys",
        "kind": 5,
        "importPath": "saver",
        "description": "saver",
        "peekOfCode": "entry_keys = []\nfor entry in entries:\n    topics_list.append(entry)\n    entry_keys.append(str(uuid.uuid4()))\nassert(type(topics_list) == list) # make sure it's a list\n# entries, entry_names = generate_entries_from_list(lore_dict['people'])\n# generate only the entries in topics_list that are not already in the lorebook\n#existing_topics = [entry['name'] for entry in lore_dict['entries']]\n#topics_list = [x for x in topics_list if x not in lore_dict]\nentries, entry_names, ids,entry_keywords = generate_entries_from_list(master_list_of_names)",
        "detail": "saver",
        "documentation": {}
    },
    {
        "label": "#existing_topics",
        "kind": 5,
        "importPath": "saver",
        "description": "saver",
        "peekOfCode": "#existing_topics = [entry['name'] for entry in lore_dict['entries']]\n#topics_list = [x for x in topics_list if x not in lore_dict]\nentries, entry_names, ids,entry_keywords = generate_entries_from_list(master_list_of_names)\n# add the entries to the lorebook dictionary. All we have to change is the text, display name, create a random id, and add the keys (which are the words in the text). All other fields can be copied from the first entry.\n# create a list of the keys for each entry (all proper nouns, places and dates)\nkeys = []\nkeys_dict = {}\nentry_id = 0\nfor entry in tqdm(entries):\n    print(f'Processing entry {entry[0:50]}...')",
        "detail": "saver",
        "documentation": {}
    },
    {
        "label": "#topics_list",
        "kind": 5,
        "importPath": "saver",
        "description": "saver",
        "peekOfCode": "#topics_list = [x for x in topics_list if x not in lore_dict]\nentries, entry_names, ids,entry_keywords = generate_entries_from_list(master_list_of_names)\n# add the entries to the lorebook dictionary. All we have to change is the text, display name, create a random id, and add the keys (which are the words in the text). All other fields can be copied from the first entry.\n# create a list of the keys for each entry (all proper nouns, places and dates)\nkeys = []\nkeys_dict = {}\nentry_id = 0\nfor entry in tqdm(entries):\n    print(f'Processing entry {entry[0:50]}...')\n    keys = [] # reset the keys list, so we don't have duplicate keys",
        "detail": "saver",
        "documentation": {}
    },
    {
        "label": "keys",
        "kind": 5,
        "importPath": "saver",
        "description": "saver",
        "peekOfCode": "keys = []\nkeys_dict = {}\nentry_id = 0\nfor entry in tqdm(entries):\n    print(f'Processing entry {entry[0:50]}...')\n    keys = [] # reset the keys list, so we don't have duplicate keys\n    for word, tag in tqdm(preprocess(entry)):\n        if (tag == 'NNP' or tag == 'NNPS' or tag == 'CD') and word not in keys\\\n            and word not in stop_words and len(word) > 2: # remove stop words, and numbers greater than 2020 (which are probably years)\n            try:",
        "detail": "saver",
        "documentation": {}
    },
    {
        "label": "keys_dict",
        "kind": 5,
        "importPath": "saver",
        "description": "saver",
        "peekOfCode": "keys_dict = {}\nentry_id = 0\nfor entry in tqdm(entries):\n    print(f'Processing entry {entry[0:50]}...')\n    keys = [] # reset the keys list, so we don't have duplicate keys\n    for word, tag in tqdm(preprocess(entry)):\n        if (tag == 'NNP' or tag == 'NNPS' or tag == 'CD') and word not in keys\\\n            and word not in stop_words and len(word) > 2: # remove stop words, and numbers greater than 2020 (which are probably years)\n            try:\n                if int(word) < 2020:",
        "detail": "saver",
        "documentation": {}
    },
    {
        "label": "entry_id",
        "kind": 5,
        "importPath": "saver",
        "description": "saver",
        "peekOfCode": "entry_id = 0\nfor entry in tqdm(entries):\n    print(f'Processing entry {entry[0:50]}...')\n    keys = [] # reset the keys list, so we don't have duplicate keys\n    for word, tag in tqdm(preprocess(entry)):\n        if (tag == 'NNP' or tag == 'NNPS' or tag == 'CD') and word not in keys\\\n            and word not in stop_words and len(word) > 2: # remove stop words, and numbers greater than 2020 (which are probably years)\n            try:\n                if int(word) < 2020:\n                    continue",
        "detail": "saver",
        "documentation": {}
    },
    {
        "label": "context_config",
        "kind": 5,
        "importPath": "saver",
        "description": "saver",
        "peekOfCode": "context_config = {\n        \"prefix\": \"\",\n        \"suffix\": \"\\n\",\n        \"tokenBudget\": 100, # max 2048\n        \"reservedTokens\": 0,\n        \"budgetPriority\": 400,\n        \"trimDirection\": \"trimBottom\",\n        \"insertionType\": \"newline\",\n        \"maximumTrimType\": \"sentence\",\n        \"insertionPosition\": -1",
        "detail": "saver",
        "documentation": {}
    }
]